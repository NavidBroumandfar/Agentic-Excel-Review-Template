# ğŸ“¦ Cursor Prompt â€” **M5: Taxonomy Manager (Label Standardization)**

**Project:** `MTCR_Agentic_Automation`
**Goal:** Build a controlled, versioned vocabulary for *"Reason for Correction"* that merges near-duplicate labels, emits YAML taxonomy snapshots, monthly drift CSVs, and an append-only JSONL change log â€” all in assistive/read-only mode with full traceability.
**Grounding:** This supports MTCR governance, sampling, and KPI follow-up described in LLDC 029014 and attachments (targets, review scope, and monthly cadence).  

---

## âœ… What to create (files & updates)

1. **`src/utils/taxonomy_manager.py`** â€” core module + CLI
2. **`tests/test_taxonomy_manager.py`** â€” unit tests using mock JSONL logs
3. **`docs/prompts/module-05.txt`** â€” save this launcher text + dev notes
4. **`src/context/ProjectVision.ts`** â€” mark M5 as *Active* and add a short changelog line
5. **`requirements.txt`** â€” ensure `pandas`, `pyyaml`, `rapidfuzz` are present

---

## âš ï¸ Compliance Header (put at top of `taxonomy_manager.py`)

```python
# âš ï¸ Compliance Notice:
# Assistive mode only. Do NOT overwrite validated cells/ranges in `MTCR Data.xlsm`.
# This module reads AI inference logs and writes ONLY new versioned outputs:
#   /data/taxonomy/reasons.vN.yml, reasons.latest.yml
#   /logs/taxonomy_drift_YYYYMM.csv
#   /logs/taxonomy_changes.jsonl
# Maintain traceability: input months, counts, timestamps, file checksums (SHA256).
```

---

## ğŸ§  Functional spec (implement in `taxonomy_manager.py`)

### Inputs

* Read reasons from **M2 logs**: `/logs/mtcr_review_assistant_YYYYMM.jsonl` (one or more months).
* Optional: merge with **M4 aggregates** if present (non-blocking).

### Processing

* **Normalize**: lowercase, trim, collapse spaces, strip punctuation; keep original string for alias.
* **Cluster** with fuzzy similarity using `rapidfuzz.fuzz.token_sort_ratio`:
  * Default `fuzzy_threshold=87` (CLI-overridable).
  * Union-find style clustering to avoid chain/hairball issues.
* **Canonical selection** per cluster:
  * Pick label with **highest frequency**, break ties by **highest avg confidence**, then **shortest length**.
* **Metrics per canonical**: count, unique aliases, avg confidence, months covered.
* **Versioning**:
  * Load `data/taxonomy/reasons.latest.yml` if exists.
  * If canonicalâ†’aliases map changed (content hash diff), **increment N** and write `reasons.v{N}.yml`, then copy to `reasons.latest.yml`.
* **Drift report** (`/logs/taxonomy_drift_YYYYMM.csv`):
  * For each processed month, output `month,alias,canonical,count,share_pct,avg_conf`.
* **Change log** (`/logs/taxonomy_changes.jsonl` append-only):
  * For each diff vs prior latest: record `{ts, action, canonical, alias, old, new, model_version?, threshold}` where `action âˆˆ {added_alias, new_canonical, removed_alias, merged_into}`.

### CLI

```
python -m src.utils.taxonomy_manager \
  --months 202510 \
  --fuzzy_threshold 87 \
  --output_yaml data/taxonomy/reasons.latest.yml \
  --drift_csv logs/taxonomy_drift_202510.csv \
  --changes_jsonl logs/taxonomy_changes.jsonl \
  --append true \
  --dry_run false \
  --verbose true
```

* `--months`: one or more `YYYYMM` values
* `--append`: append to changes log (default true)
* `--dry_run`: compute only; do not write files
* `--verbose`: print compact YAML summary to stdout

### Traceability

* Compute and embed SHA256 of each input file and the produced YAML into:
  * YAML header comments
  * First row of drift CSV (as metadata row with `#` prefix)
  * Each JSONL event (`input_checksums`, `output_checksum`)

---

## ğŸ§ª Tests (`tests/test_taxonomy_manager.py`)

Scenarios (use `tmp_path`):

1. Create mock `/logs/mtcr_review_assistant_202510.jsonl` with variants:
   * "Wrong Error Code", "Error code incorrect", "Incorrect error-code", "Wrong   error  code"
   * Include `confidence` floats.
2. Run CLI (`--months 202510 --dry_run false`) and assert:
   * `data/taxonomy/reasons.latest.yml` exists and contains **one canonical** with **aliases**.
   * `logs/taxonomy_drift_202510.csv` exists; rows include alias counts and share.
   * `logs/taxonomy_changes.jsonl` appended (not empty).
3. Add a new variant ("Error code is wrong") â†’ re-run â†’ assert **version N+1** is created and `latest` updated.
4. `--dry_run true` produces **no file changes**.
5. `--fuzzy_threshold` higher causes split clusters (assert 2 canonicals).

---

## ğŸ§± Code to generate

### `src/utils/taxonomy_manager.py`

```python
import argparse
import hashlib
import json
import os
import re
import sys
from dataclasses import dataclass, field
from datetime import datetime, timezone
from pathlib import Path
from typing import Dict, List, Tuple, Iterable

import pandas as pd
from rapidfuzz import fuzz
import yaml

# âš ï¸ Compliance Notice:
# Assistive mode only. Do NOT overwrite validated cells/ranges in `MTCR Data.xlsm`.
# Writes only new files: versioned YAML, drift CSV, append-only JSONL. Full traceability.

REASON_COL = "reason"           # from M2 schema
CONF_COL = "confidence"         # 0..1
MODEL_VER_COL = "model_version" # optional in logs

NORMALIZE_RE = re.compile(r"[^\w\s]", flags=re.UNICODE)

def sha256_file(path: Path) -> str:
    h = hashlib.sha256()
    with path.open("rb") as f:
        for chunk in iter(lambda: f.read(65536), b""):
            h.update(chunk)
    return h.hexdigest()

def sha256_text(text: str) -> str:
    return hashlib.sha256(text.encode("utf-8")).hexdigest()

def normalize_reason(s: str) -> str:
    s = s.strip().lower()
    s = NORMALIZE_RE.sub(" ", s)
    s = re.sub(r"\s+", " ", s)
    return s

@dataclass
class ReasonSample:
    raw: str
    norm: str
    conf: float
    month: str
    model_version: str | None = None

@dataclass
class Cluster:
    canonical: str = ""
    members: List[str] = field(default_factory=list)

def read_month_logs(months: List[str], logs_dir: Path) -> List[ReasonSample]:
    samples: List[ReasonSample] = []
    for m in months:
        fp = logs_dir / f"mtcr_review_assistant_{m}.jsonl"
        if not fp.exists():
            continue
        with fp.open("r", encoding="utf-8") as f:
            for line in f:
                try:
                    obj = json.loads(line)
                except json.JSONDecodeError:
                    continue
                reason = (obj.get(REASON_COL) or "").strip()
                if not reason:
                    continue
                conf = float(obj.get(CONF_COL, 0.0))
                mv = obj.get(MODEL_VER_COL)
                samples.append(
                    ReasonSample(raw=reason, norm=normalize_reason(reason), conf=conf, month=m, model_version=mv)
                )
    return samples

def cluster_reasons(samples: List[ReasonSample], threshold: int = 87) -> Dict[str, List[str]]:
    """Return {canonical -> [aliases]} (aliases include canonical itself if variants exist)."""
    # Build buckets by normalized form first to avoid O(n^2) where obvious duplicates exist
    uniq_norms = {}
    for s in samples:
        uniq_norms.setdefault(s.norm, set()).add(s.raw)

    norms = list(uniq_norms.keys())
    parent = {i: i for i in range(len(norms))}

    def find(i):
        while parent[i] != i:
            parent[i] = parent[parent[i]]
            i = parent[i]
        return i

    def union(i, j):
        ri, rj = find(i), find(j)
        if ri != rj:
            parent[rj] = ri

    # Union by fuzzy similarity
    for i in range(len(norms)):
        for j in range(i + 1, len(norms)):
            score = fuzz.token_sort_ratio(norms[i], norms[j])
            if score >= threshold:
                union(i, j)

    # Gather clusters by root
    clusters: Dict[int, List[str]] = {}
    for idx, norm in enumerate(norms):
        r = find(idx)
        clusters.setdefault(r, []).append(norm)

    # Choose canonical per cluster using frequency -> avg confidence -> shortest length
    freq: Dict[str, int] = {}
    conf_sum: Dict[str, float] = {}
    for s in samples:
        freq[s.norm] = freq.get(s.norm, 0) + 1
        conf_sum[s.norm] = conf_sum.get(s.norm, 0.0) + float(s.conf or 0.0)

    result: Dict[str, List[str]] = {}
    for _, norm_list in clusters.items():
        def key_fn(n):
            return (freq.get(n, 0), conf_sum.get(n, 0.0) / max(freq.get(n, 1), 1), -len(n))
        canon_norm = sorted(norm_list, key=key_fn, reverse=True)[0]
        # collect all raw variants under the norms in this cluster
        alias_raws = []
        for n in norm_list:
            alias_raws.extend(sorted(uniq_norms[n]))
        # ensure stable order & uniqueness
        seen = set()
        ordered_aliases = [a for a in alias_raws if (a not in seen and not seen.add(a))]
        result[canon_norm] = ordered_aliases
    return result

def build_yaml_payload(canon_to_aliases: Dict[str, List[str]], samples: List[ReasonSample]) -> dict:
    # compute metrics
    df = pd.DataFrame([{"norm": s.norm, "raw": s.raw, "conf": s.conf, "month": s.month} for s in samples])
    items = []
    for canon_norm, aliases in canon_to_aliases.items():
        sub = df[df["norm"].isin([normalize_reason(a) for a in aliases])]
        items.append({
            "canonical": aliases[0] if aliases else canon_norm,  # present best raw for canon
            "canonical_norm": canon_norm,
            "aliases": sorted(set(aliases) - {aliases[0]}),
            "metrics": {
                "count": int(len(sub)),
                "avg_conf": float(sub["conf"].mean()) if len(sub) else 0.0,
                "months": sorted(sub["month"].unique().tolist())
            }
        })
    # stable sort by descending count then canonical alpha
    items.sort(key=lambda x: (x["metrics"]["count"], x["canonical"].lower()), reverse=True)
    return {
        "schema": "mtcr.taxonomy.reasons/1-0-0",
        "generated_at": datetime.now(timezone.utc).isoformat(),
        "items": items
    }

def load_latest_yaml(path_latest: Path) -> dict | None:
    if not path_latest.exists():
        return None
    with path_latest.open("r", encoding="utf-8") as f:
        return yaml.safe_load(f) or {}

def yaml_content_hash(payload: dict) -> str:
    # dump canonical with sorted keys for stable hash
    text = yaml.safe_dump(payload, sort_keys=True, allow_unicode=True)
    return sha256_text(text)

def write_versioned_yaml(payload: dict, out_latest: Path) -> Tuple[Path, str, int]:
    out_dir = out_latest.parent
    out_dir.mkdir(parents=True, exist_ok=True)
    # detect next version
    existing = sorted(out_dir.glob("reasons.v*.yml"))
    next_v = 1
    if existing:
        max_v = max(int(p.stem.split("v")[-1]) for p in existing if "v" in p.stem)
        next_v = max_v + 1
    versioned = out_dir / f"reasons.v{next_v}.yml"
    # add header comment with checksum
    text = yaml.safe_dump(payload, sort_keys=False, allow_unicode=True)
    checksum = sha256_text(text)
    header = f"# file_checksum_sha256: {checksum}\n"
    versioned.write_text(header + text, encoding="utf-8")
    out_latest.write_text(header + text, encoding="utf-8")
    return versioned, checksum, next_v

def diff_taxonomies(old: dict | None, new: dict) -> List[dict]:
    # naive diff at alias level per canonical_norm
    changes = []
    old_map = {}
    if old:
        for it in old.get("items", []):
            old_map[it["canonical_norm"]] = set([it["canonical"], *it.get("aliases", [])])
    for it in new.get("items", []):
        cn = it["canonical_norm"]
        new_set = set([it["canonical"], *it.get("aliases", [])])
        if cn not in old_map:
            changes.append({"action": "new_canonical", "canonical": it["canonical"]})
        else:
            added = sorted(new_set - old_map[cn])
            removed = sorted(old_map[cn] - new_set)
            for a in added:
                changes.append({"action": "added_alias", "canonical": it["canonical"], "alias": a})
            for a in removed:
                changes.append({"action": "removed_alias", "canonical": it["canonical"], "alias": a})
    return changes

def write_drift_csv(month: str, canon_to_aliases: Dict[str, List[str]], samples: List[ReasonSample], out_csv: Path, input_checksums: Dict[str, str]):
    out_csv.parent.mkdir(parents=True, exist_ok=True)
    df = pd.DataFrame([{"alias": s.raw, "norm": s.norm, "conf": s.conf, "month": s.month} for s in samples if s.month == month])
    rows = []
    total = len(df)
    for canon_norm, aliases in canon_to_aliases.items():
        for alias in aliases:
            sub = df[df["alias"] == alias]
            if sub.empty: 
                continue
            count = int(len(sub))
            share = round(100.0 * count / max(total, 1), 2)
            rows.append({
                "month": month,
                "alias": alias,
                "canonical_norm": canon_norm,
                "count": count,
                "share_pct": share,
                "avg_conf": float(sub["conf"].mean())
            })
    meta = "# input_checksums=" + json.dumps(input_checksums)
    with out_csv.open("w", encoding="utf-8", newline="") as f:
        f.write(meta + "\n")
        pd.DataFrame(rows).to_csv(f, index=False)

def append_changes_log(changes: List[dict], out_jsonl: Path, append: bool, context: dict):
    if not append or not changes:
        return
    out_jsonl.parent.mkdir(parents=True, exist_ok=True)
    ts = datetime.now(timezone.utc).isoformat()
    with out_jsonl.open("a", encoding="utf-8") as f:
        for ch in changes:
            rec = {"ts": ts, **context, **ch}
            f.write(json.dumps(rec, ensure_ascii=False) + "\n")

def main(argv=None):
    p = argparse.ArgumentParser(prog="taxonomy_manager", description="MTCR Reason-for-Correction Taxonomy Manager")
    p.add_argument("--months", nargs="+", required=True)
    p.add_argument("--fuzzy_threshold", type=int, default=87)
    p.add_argument("--output_yaml", type=Path, default=Path("data/taxonomy/reasons.latest.yml"))
    p.add_argument("--drift_csv", type=Path, required=True)
    p.add_argument("--changes_jsonl", type=Path, default=Path("logs/taxonomy_changes.jsonl"))
    p.add_argument("--append", type=lambda x: str(x).lower() != "false", default=True)
    p.add_argument("--dry_run", type=lambda x: str(x).lower() == "true", default=False)
    p.add_argument("--verbose", type=lambda x: str(x).lower() == "true", default=False)
    p.add_argument("--logs_dir", type=Path, default=Path("logs"))
    args = p.parse_args(argv)

    samples = read_month_logs(args.months, args.logs_dir)
    if not samples:
        print("No samples found for given months.", file=sys.stderr)
        return 1

    input_checksums = {}
    for m in args.months:
        fp = args.logs_dir / f"mtcr_review_assistant_{m}.jsonl"
        if fp.exists():
            input_checksums[m] = sha256_file(fp)

    clusters = cluster_reasons(samples, args.fuzzy_threshold)
    payload = build_yaml_payload(clusters, samples)

    old_latest = load_latest_yaml(args.output_yaml)
    changed = (yaml_content_hash(old_latest or {}) != yaml_content_hash(payload))

    if args.verbose:
        print(yaml.safe_dump(payload, sort_keys=False, allow_unicode=True))

    if not args.dry_run:
        if changed:
            versioned_path, out_checksum, vnum = write_versioned_yaml(payload, args.output_yaml)
        else:
            # mirror latest write to embed checksum header even if unchanged
            text = yaml.safe_dump(payload, sort_keys=False, allow_unicode=True)
            out_checksum = sha256_text(text)
            header = f"# file_checksum_sha256: {out_checksum}\n"
            args.output_yaml.parent.mkdir(parents=True, exist_ok=True)
            args.output_yaml.write_text(header + text, encoding="utf-8")
            versioned_path, vnum = None, None

        # Drift per requested month (write one CSV per invocation param)
        write_drift_csv(args.months[-1], clusters, samples, args.drift_csv, input_checksums)

        # Changes log (append)
        diffs = diff_taxonomies(old_latest, payload)
        ctx = {
            "fuzzy_threshold": args.fuzzy_threshold,
            "input_checksums": input_checksums,
            "output_checksum": out_checksum,
            "months": args.months
        }
        append_changes_log(diffs, args.changes_jsonl, args.append, ctx)

    return 0

if __name__ == "__main__":
    raise SystemExit(main())
```

### `tests/test_taxonomy_manager.py`

```python
import json
from pathlib import Path
import subprocess
import sys
import yaml
import pytest

def write_jsonl(p: Path, rows: list[dict]):
    p.parent.mkdir(parents=True, exist_ok=True)
    with p.open("w", encoding="utf-8") as f:
        for r in rows:
            f.write(json.dumps(r) + "\n")

def run_cli(args, cwd: Path):
    cmd = [sys.executable, "-m", "src.utils.taxonomy_manager", *args]
    return subprocess.run(cmd, cwd=cwd, check=False, capture_output=True, text=True)

def test_taxonomy_end_to_end(tmp_path: Path):
    # Arrange
    logs = tmp_path / "logs"
    write_jsonl(logs / "mtcr_review_assistant_202510.jsonl", [
        {"reason": "Wrong Error Code", "confidence": 0.72},
        {"reason": "Error code incorrect", "confidence": 0.8},
        {"reason": "Incorrect error-code", "confidence": 0.7},
        {"reason": "Wrong   error  code", "confidence": 0.75},
    ])

    # Act
    args = [
        "--months","202510",
        "--fuzzy_threshold","87",
        "--output_yaml","data/taxonomy/reasons.latest.yml",
        "--drift_csv","logs/taxonomy_drift_202510.csv",
        "--changes_jsonl","logs/taxonomy_changes.jsonl",
        "--append","true",
        "--dry_run","false",
        "--verbose","false",
        "--logs_dir", str(logs)
    ]
    r = run_cli(args, tmp_path)
    assert r.returncode == 0, r.stderr

    # Assert YAML exists and structure looks right
    latest = tmp_path / "data/taxonomy/reasons.latest.yml"
    assert latest.exists()
    data = yaml.safe_load(latest.read_text(encoding="utf-8"))
    items = data.get("items", [])
    assert len(items) == 1
    assert "canonical" in items[0]
    assert items[0]["aliases"]  # has some aliases

    # Drift CSV
    drift = tmp_path / "logs/taxonomy_drift_202510.csv"
    assert drift.exists()
    txt = drift.read_text(encoding="utf-8")
    assert "alias,canonical_norm,count,share_pct,avg_conf" in txt

    # Changes log appended
    changes = tmp_path / "logs/taxonomy_changes.jsonl"
    assert changes.exists()
    assert changes.read_text(encoding="utf-8").strip() != ""

    # Run again with a new variant -> version bump
    with (logs / "mtcr_review_assistant_202510.jsonl").open("a", encoding="utf-8") as f:
        f.write(json.dumps({"reason":"Error code is wrong","confidence":0.77})+"\n")
    r2 = run_cli(args, tmp_path)
    assert r2.returncode == 0
    # a new versioned file should exist
    versions = sorted((tmp_path / "data/taxonomy").glob("reasons.v*.yml"))
    assert len(versions) >= 1
```

---

## ğŸ›  Update `requirements.txt`

Ensure these lines exist (add if missing):

```
pandas
pyyaml
rapidfuzz
```

---

## ğŸ—‚ Update `src/context/ProjectVision.ts`

* Set **M5 â€“ Taxonomy Manager** to *Active*.
* Add a one-line changelog:
  `2025-10-29: M5 initialized â€” taxonomy clustering, versioned YAML, drift CSV, JSONL changes.`

---

## ğŸ§ª How to run locally

```bash
# build taxonomy for Oct 2025 (example)
python -m src.utils.taxonomy_manager \
  --months 202510 \
  --fuzzy_threshold 87 \
  --output_yaml data/taxonomy/reasons.latest.yml \
  --drift_csv logs/taxonomy_drift_202510.csv \
  --changes_jsonl logs/taxonomy_changes.jsonl \
  --append true \
  --dry_run false \
  --verbose true

# run tests
pytest -q
```

---

## ğŸ“ Notes for maintainers

* This module **does not** touch `MTCR Data.xlsm` (assistive only).
* It aligns with monthly MTCR cadence and KPI governance described in SOP 029014 and attachments (sampling, dashboard checks, approvals).  

---

## ğŸ”§ Development Notes

### Implementation Status
- âœ… Core taxonomy manager module created
- âœ… Comprehensive test suite with 8 test scenarios
- âœ… Fuzzy clustering with configurable threshold
- âœ… Versioned YAML output with checksums
- âœ… Drift CSV reporting
- âœ… Append-only JSONL change log
- âœ… Full traceability with SHA256 checksums

### Key Features
1. **Fuzzy Clustering**: Uses `rapidfuzz.fuzz.token_sort_ratio` for similarity detection
2. **Canonical Selection**: Prioritizes frequency â†’ confidence â†’ length
3. **Versioning**: Auto-increments version numbers when taxonomy changes
4. **Traceability**: SHA256 checksums embedded in all outputs
5. **Compliance**: Read-only mode, no modification of source Excel files

### Test Coverage
- End-to-end workflow testing
- Version bumping on new variants
- Dry run mode validation
- Fuzzy threshold behavior
- Checksum traceability
- Empty input handling
- Multiple month processing
- Confidence/frequency ranking

### CLI Usage Examples
```bash
# Basic usage
python -m src.utils.taxonomy_manager --months 202510 --drift_csv logs/drift.csv

# High precision clustering
python -m src.utils.taxonomy_manager --months 202510 --fuzzy_threshold 95 --drift_csv logs/drift.csv

# Dry run to preview changes
python -m src.utils.taxonomy_manager --months 202510 --dry_run true --drift_csv logs/drift.csv

# Multiple months
python -m src.utils.taxonomy_manager --months 202510 202511 --drift_csv logs/drift.csv
```

If anything fails, print the CLI help and still generate partial outputs when safe.
