# ğŸš€ Launcher Prompt â€” Phase 6 (M6): SOP Indexer (RAG + Guidance Mapping)

> ğŸ’¬ **Chat Title:**
> ğŸ¤– MTCR_Agentic_Automation â€” Phase 6 Â· SOP Indexer (RAG + Guidance Mapping)

---

## ğŸ“˜ Objective

Build a **local RAG index** over SOP **029014** and attachments, then expose a function that maps each **standardized "Reason for Correction"** (from M5) to the best SOP clause(s) with similarity score, excerpt, and traceability (checksums, timestamps, model version).

> Note: SOP 029014 governs the MTCR process and defines roles, monthly review activities, and dashboard rules (e.g., fields to review, KPI targets), which are the ground truth we must reference in mapping. 
> The dashboard error definitions and correction guidance live in **Attachment 3** (Self-Review User Guide). 
> Sampling plan rules are in **Attachment 2** (Golden Rules). 
> CHP Governance ops cadence and deliverables appear in **CHP Activities / Mode d'emploi**.

---

## ğŸ“‚ File paths

* **Inputs (local, read-only):**

  * `/mnt/data/029014 - Periodic Technical Complaints Review.pdf`
  * `/mnt/data/029014 - Attachment 3 - TC Self-Review Dashboard User Guide.pdf`
  * `/mnt/data/029014 - Attachment 2 - Golden Rules for Determination of Sampling Plan.docx`
  * `/mnt/data/Attachment 1 - MTCR - CHP Activities.docx`
  * `/mnt/data/Attachment 2 - MTCR - Mode d'emploi.docx`
* **Embeddings DB:** `data/embeddings/sop_index/`
* **Taxonomy (from M5):** `data/taxonomy/reasons.latest.yml` (input)
* **Outputs:**

  * `data/mappings/reason_to_sop.yml`
  * `logs/rag_mapping_YYYYMM.jsonl`

---

## ğŸ”— Modules to Create/Update

**Create**

1. `src/utils/sop_indexer.py` â€“ *RAG index build + retrieval + mapping generator*
2. `tests/test_sop_indexer.py` â€“ *unit tests with mock corpus*

**Update**

1. `requirements.txt` â€“ add: `chromadb` **or** `faiss-cpu`, `sentence-transformers`, `pandas`, `pyyaml`, `python-docx`, `pypdf`
2. `docs/prompts/module-06.txt` â€“ save this prompt
3. `src/context/ProjectVision.ts` â€“ mark **M6 Active**, note RAG integration

---

## ğŸ§± Implementation Notes (Senthira Flow style)

### ğŸ”— Dependencies

* Vector store: **Chroma** (default). Allow `--use_faiss` flag to switch.
* Embeddings: `sentence-transformers/all-MiniLM-L6-v2` (local). Expose CLI arg `--embedding_model`.
* Parsers: `pypdf` for PDFs, `python-docx` for DOCX. For XLSX attachments (if any), just capture filenames + checksums in metadata (no parsing needed this phase).

### ğŸ§  Chunking & Metadata

Split into semantically meaningful **clauses** using:

* Heading regex: `^(?:\d+(?:\.\d+)*\s+|Attachment\s+\d+\s+-\s+|Definitions|Scope|Responsibilities|Monthly Technical Complaints Review)` (multiline).
* Max chunk ~1,200â€“1,600 chars, **preserve headings** and **section IDs** (e.g., "4.1.3").
* **Metadata per chunk:**

  ```json
  {
    "doc_id": "029014",
    "title": "Periodic Technical Complaints Review",
    "section": "4.1.3",
    "section_title": "Monthly quality content review",
    "page": 6,
    "filepath": "...",
    "checksum_sha256": "...",
    "rev": "15.A",
    "effective_date": "2024-10-01T01:00:00",
    "source_type": "pdf|docx",
    "created_at": "ISO8601",
    "model": "all-MiniLM-L6-v2"
  }
  ```

### ğŸ” Retrieval API

```python
def find_relevant_sop(reason_text: str, top_k: int = 3) -> list[dict]:
    """
    Returns: [{section, section_title, sop_ref, excerpt, score, doc_id, page, rev}]
    """
```

* `sop_ref` format: `"029014 Â§4.1.3"` or `"Attachment 3 â€“ PRE Not escalated"`.
* Use cosine similarity; include raw score.

### ğŸ” Mapping Generator

* Ingest canonical reasons from **M5 YAML** (`data/taxonomy/reasons.latest.yml`):

  ```yaml
  - id: R_ERRCODE
    label: Incorrect Error Code
    synonyms: ["wrong IMDRF A-code", "mis-coded error"]
  ```
* For each `label` (and synonyms), call retrieval, take **top-1** (and keep **top-3** in logs).
* Write `data/mappings/reason_to_sop.yml`:

  ```yaml
  - reason: Incorrect Error Code
    sop_ref: "029014 Â§4.1.3"
    excerpt: "The review is limited to the evaluation... Error Code should be the most appropriate..."
    similarity: 0.91
    metadata:
      doc_id: "029014"
      rev: "15.A"
      page: 6
  ```
* **JSONL log** `logs/rag_mapping_YYYYMM.jsonl` per reason with `{ts, reason, topk:[...], chosen_idx, model, thresholds, checksums}`.

> Hints:
> â€“ Fields commonly reviewed and their "what/why" live in Â§4.1.3 table (e.g., Error Code expectation). 
> â€“ Self-Review dashboard error types and fixes (e.g., UDI-DI missing, PRE not escalated, misleading product info) are explained in Attachment 3â€”use those chunks to back reasons about coding/content quality. 
> â€“ Sampling rules and targets are in Attachment 2; retain but **do not** reinterpret. 

---

## ğŸ§ª Tests (`tests/test_sop_indexer.py`)

1. **Index Build (mock):** Create a tiny in-memory corpus with 3 faux "sections" (include a fake "Â§4.1.3 Error Code â€¦"). Assert Chroma collection has >0 docs and metadata keys present.
2. **Query:** `find_relevant_sop("sampling plan error")` returns a top hit referencing **Attachment 2** (sampling rules). 
3. **Mapping:** Provide a mini taxonomy YAML with `Incorrect Error Code`. Generate mapping; assert YAML keys exist and similarity in `[0,1]`.
4. **Audit:** After mapping, `logs/rag_mapping_YYYYMM.jsonl` contains one record per reason with `checksums`, `model`, `topk`.
5. **Idempotency:** Re-running with same inputs does not duplicate embeddings (detect by checksum index).

---

## ğŸ–¥ï¸ CLI

```bash
python -m src.utils.sop_indexer \
  --input_docs "/mnt/data/029014 - Periodic Technical Complaints Review.pdf" \
               "/mnt/data/029014 - Attachment 3 - TC Self-Review Dashboard User Guide.pdf" \
               "/mnt/data/029014 - Attachment 2 - Golden Rules for Determination of Sampling Plan.docx" \
               "/mnt/data/Attachment 1 - MTCR - CHP Activities.docx" \
               "/mnt/data/Attachment 2 - MTCR - Mode d'emploi.docx" \
  --taxonomy_yaml data/taxonomy/reasons.latest.yml \
  --embeddings_dir data/embeddings/sop_index/ \
  --output_mapping data/mappings/reason_to_sop.yml \
  --log_file logs/rag_mapping_202510.jsonl \
  --embedding_model all-MiniLM-L6-v2 \
  --top_k 3 \
  --use_faiss false \
  --verbose true
```

---

## ğŸ§  Key Functions (to implement in `src/utils/sop_indexer.py`)

* `load_docs(paths: list[str]) -> list[RawDoc]`
* `chunk_and_clean(docs) -> list[Chunk]`  *(regex headings, maxlen, carry section ids)*
* `embed_and_index(chunks, embeddings_dir, model) -> VectorStoreHandle`
* `find_relevant_sop(reason_text: str, top_k=3) -> list[dict]`
* `generate_reason_mapping(taxonomy_yaml, output_path, log_path, top_k=3) -> None`
* `compute_sha256(filepath) -> str`
* `cli()` â€“ argparse wrapper for all of the above

---

## âš ï¸ Compliance Notice

```
# Assistive mode only. Do NOT modify official SOP text.
# Index is local; outputs are new, versioned artifacts.
# Keep per-file checksum, revision (e.g., 15.A), and effective date metadata.
# Respect validated Excel/MTCR macros: read-only.
```

---

## âœ… Acceptance Criteria

* [ ] Local vector index exists at `data/embeddings/sop_index/` (deterministic rebuilds via checksums).
* [ ] `find_relevant_sop()` returns structured results with `sop_ref`, `excerpt`, `score`.
* [ ] `data/mappings/reason_to_sop.yml` created for all canonical reasons (from M5).
* [ ] `logs/rag_mapping_YYYYMM.jsonl` records each mapping decision (top-k, chosen, thresholds, checksums).
* [ ] Tests pass with mock corpus.

---

## ğŸ“¦ requirements.txt (append)

```
chromadb==0.5.*
faiss-cpu==1.8.*        # optional; use if --use_faiss true
sentence-transformers==3.*
pandas>=2.2
pyyaml>=6.0
pypdf>=4.2
python-docx>=1.1
```

---

## ğŸ“ After Merge

* Save this prompt as `docs/prompts/module-06.txt`.
* Update `src/context/ProjectVision.ts` â†’ **M6 Active** with: "SOP RAG index built; reasonâ†’clause mapping exposed; logs enabled."

---
